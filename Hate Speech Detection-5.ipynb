{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNnlrM3G2ZI43VtwFlFSXTI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3oP9TLGx6jUv","executionInfo":{"status":"ok","timestamp":1687503774682,"user_tz":-330,"elapsed":2380701,"user":{"displayName":"RASHEDIN","userId":"15453162067441304532"}},"outputId":"d64c64fd-d667-423f-a096-e284aeb8521b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","620/620 [==============================] - 236s 374ms/step - loss: 0.4808 - accuracy: 0.8210\n","Epoch 2/10\n","620/620 [==============================] - 231s 373ms/step - loss: 0.3181 - accuracy: 0.8884\n","Epoch 3/10\n","620/620 [==============================] - 232s 374ms/step - loss: 0.2304 - accuracy: 0.9203\n","Epoch 4/10\n","620/620 [==============================] - 229s 369ms/step - loss: 0.1695 - accuracy: 0.9424\n","Epoch 5/10\n","620/620 [==============================] - 231s 373ms/step - loss: 0.1254 - accuracy: 0.9573\n","Epoch 6/10\n","620/620 [==============================] - 229s 369ms/step - loss: 0.1076 - accuracy: 0.9639\n","Epoch 7/10\n","620/620 [==============================] - 229s 370ms/step - loss: 0.0872 - accuracy: 0.9699\n","Epoch 8/10\n","620/620 [==============================] - 232s 375ms/step - loss: 0.0813 - accuracy: 0.9728\n","Epoch 9/10\n","620/620 [==============================] - 235s 379ms/step - loss: 0.0645 - accuracy: 0.9788\n","Epoch 10/10\n","620/620 [==============================] - 234s 377ms/step - loss: 0.0598 - accuracy: 0.9797\n","1/1 [==============================] - 1s 1s/step\n","Predicted Class: Hate Speech\n"]}],"source":["import pandas as pd\n","import nltk\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","import re\n","\n","# Load the dataset\n","data = pd.read_csv('/content/Hate Speech Detection in Arabic Urdu -  labeled_data.csv.csv')\n","\n","# Preprocess the text\n","def preprocess(text):\n","    # Apply any necessary preprocessing steps such as removing special characters, normalization, etc.\n","    urdu_words_only = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)\n","    return urdu_words_only.strip()\n","\n","data['Tweet'] = data['Tweet'].apply(preprocess)\n","\n","data[\"Labels\"] = data[\"Class\"].map({0:\"Hate Speech\", 1:\"Offensive Language\", 2:\"No hate and Offensive Speech\"})\n","\n","# Split the data into training and test sets\n","X = data['Tweet']\n","y = data['Labels']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Tokenize the text\n","tokenizer = nltk.tokenize.WordPunctTokenizer()\n","X_train_tokens = [tokenizer.tokenize(text) for text in X_train]\n","X_test_tokens = [tokenizer.tokenize(text) for text in X_test]\n","\n","# Convert labels to categorical\n","label_encoder = LabelEncoder()\n","y_train = y_train.astype(str)\n","y_test = y_test.astype(str)\n","y_train_encoded = label_encoder.fit_transform(y_train)\n","y_test_encoded = label_encoder.transform(y_test)\n","num_classes = len(label_encoder.classes_)\n","\n","# Convert tokens to sequences\n","max_sequence_length = 100  # Define the maximum sequence length\n","# Convert tokens to sequences and pad them\n","tokenizer = nltk.tokenize.WordPunctTokenizer()\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","# Tokenize the text\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(X_train)\n","\n","X_train_sequences = tokenizer.texts_to_sequences(X_train)\n","X_test_sequences = tokenizer.texts_to_sequences(X_test)\n","\n","# Convert tokens to sequences and pad them\n","X_train_sequences = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post')\n","X_test_sequences = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post')\n","\n","# Update the vocabulary size\n","vocabulary_size = len(tokenizer.word_index) + 1\n","\n","# Define the model architecture (Bidirectional LSTM in this example)\n","model = Sequential()\n","model.add(Embedding(input_dim=vocabulary_size, output_dim=100, input_length=max_sequence_length))\n","model.add(Bidirectional(LSTM(units=128)))\n","model.add(Dense(units=num_classes, activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(X_train_sequences, to_categorical(y_train_encoded, num_classes=num_classes), epochs=10, batch_size=32)\n","\n"]},{"cell_type":"code","source":["\n","# Make predictions\n","text = \"آپ کیسے ہو\"  # The text you want to classify\n","\n","# Preprocess the text\n","preprocessed_text = preprocess(text)\n","\n","# Tokenize and pad the preprocessed text\n","text_tokens = tokenizer.texts_to_sequences([preprocessed_text])\n","text_tokens_padded = pad_sequences(text_tokens, maxlen=max_sequence_length, padding='post')\n","\n","# Make prediction\n","prediction_probs = model.predict(text_tokens_padded)\n","predicted_class_index = prediction_probs.argmax(axis=1)[0]\n","\n","# Map the predicted class index to the corresponding label\n","predicted_class = label_encoder.inverse_transform([predicted_class_index])[0]\n","\n","# Print the predicted class\n","# print(\"Predicted Class:\", predicted_class)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zwNQS6m0EHep","executionInfo":{"status":"ok","timestamp":1687503880350,"user_tz":-330,"elapsed":403,"user":{"displayName":"RASHEDIN","userId":"15453162067441304532"}},"outputId":"ef38665e-54e3-4997-afb8-1b2be52f1f69"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 96ms/step\n"]}]},{"cell_type":"code","source":["if \"Hate Speech\" in predicted_class:\n","    print(\"The text is considered hateful.\")\n","elif \"Offensive Language\" in predicted_class:\n","    print(\"The text is considered offensive.\")\n","elif \"No hate and Offensive Speech\" in predicted_class:\n","    print(\"The text is not considered hateful or offensive.\")\n","else:\n","    print(\"Unknown class.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2CcDsPpfEH30","executionInfo":{"status":"ok","timestamp":1687503882584,"user_tz":-330,"elapsed":4,"user":{"displayName":"RASHEDIN","userId":"15453162067441304532"}},"outputId":"88a0ca80-89b3-442f-b28b-00bb24860c53"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["The text is considered offensive.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"hdR-36elESET"},"execution_count":null,"outputs":[]}]}